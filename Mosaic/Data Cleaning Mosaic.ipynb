{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through a necessary step of any data science project - data cleaning. Data cleaning is a time consuming and unenjoyable task, yet it's a very important one. Keep in mind, \"garbage in, garbage out\". Feeding dirty data into a model will give us results that are meaningless.\n",
    "\n",
    "Specifically, we'll be walking through:\n",
    "\n",
    "1. **Getting the data - **in this case, we'll be scraping data from a website\n",
    "2. **Cleaning the data - **we will walk through popular text pre-processing techniques\n",
    "3. **Organizing the data - **we will organize the cleaned data into a way that is easy to input into other algorithms\n",
    "\n",
    "The output of this notebook will be clean, organized data in two standard text formats:\n",
    "\n",
    "1. **Corpus** - a collection of text\n",
    "2. **Document-Term Matrix** - word counts in matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, our goal is to look at transcripts of various comedians and note their similarities and differences. Specifically, I'd like to know if Ali Wong's comedy style is different than other comedians, since she's the comedian that got me interested in stand up comedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there are wonderful people online that keep track of stand up routine transcripts. [Scraps From The Loft](http://scrapsfromtheloft.com) makes them available for non-profit and educational purposes.\n",
    "\n",
    "To decide which comedians to look into, I went on IMDB and looked specifically at comedy specials that were released in the past 5 years. To narrow it down further, I looked only at those with greater than a 7.5/10 rating and more than 2000 votes. If a comedian had multiple specials that fit those requirements, I would pick the most highly rated one. I ended up with a dozen comedy specials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping, pickle imports\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Load pickled files\n",
    "# with open(\"funhouse.csv\", \"r\", encoding='utf-8') as file:\n",
    "#      corpus = str(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  with open('phrases.csv', 'w', encoding='utf-8') as file:\n",
    "#     for word in corpus.split(','):\n",
    "#         file.write(str(word))\n",
    "#         file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('/\\s/g', '', text)\n",
    "    #text = deEmojify(text)\n",
    "    text = re.sub(\"[^\\x00-\\x7F]+\", '', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8853309154510498\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "with open('round1.txt', 'r', encoding='utf-8') as file:\n",
    "    with open('round1_clean','w', encoding = 'utf-8') as output:\n",
    "        for i in file.readlines():\n",
    "            output.write(clean(i))\n",
    "            output.write('\\n')\n",
    "\n",
    "print(time.time() - stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4999806880950928\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "with open('phrases.csv', 'r', encoding='utf-8') as file:\n",
    "    with open('cleanPhrase.csv','w', encoding = 'utf-8') as output:\n",
    "        for i in file.readlines():\n",
    "            output.write(clean(i))\n",
    "            output.write('\\n')\n",
    "\n",
    "print(time.time() - stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('cleanPhrase.csv')\n",
    "df.to_csv('output.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45969295501708984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syao1\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stime = time.time()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "corpus = open('r1.txt')\n",
    "data_cv = cv.fit_transform(corpus)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names()).transpose()\n",
    "data_dtm.to_csv('funhouse_freq.csv')\n",
    "\n",
    "print(time.time() - stime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our data again\n",
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: comedian, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
